---
layout: post
title:  "Forword-Looking Active REtrieval augmented generation"
tags:
  - FLARE
  - LLM
hero: https://source.unsplash.com/collection/430471/
overlay: red
published: true
use_math: true
---

FLARE (Forward-Looking Active Retrieval augmented generation) is a method that enhances long-form text generation by actively retrieving relevant information throughout the generation process. It anticipates future content needs and retrieves documents to regenerate low-confidence sentences, improving the accuracy and reliability of generated text.
{: .lead}

LM(Large Language Model)의 문제점:
	1. Hallucination
	2. create imaginary content
<!–-break-–>


그래서 나온게 Retrieval augmented LM
=> retrieve-and-generate setup
유저의 입력에 따라 문서를 retrieve 하고 받은 문서에 기반해 답변을 generate
이게 pure한 LM에 비해 확실히 성능은 더 좋음. 특히 짧은 정보 축약적인 답변 생성에 있어서는.

LLM의 성능 향상은 Long-form output 을 생성하거나 open-domain 요약, 추론 등의 복잡한 작업을 실행하는 능력을 얻게 되었다.
짧은 형태가 아니라 긴 형태의 답변을 생성하는 데에 있어서 복잡한 정보를 필요로 한다. input으로 주어지지 않을 수도 있는. 그래서 사람이 정보를 얻는 것처럼 다른 곳에서 정보를 얻어오는 과정을 필요로 한다.

예를 들어 어떤 주제에 대한 요약을 만들어 내는데에 있어서 첫 번째 retrieval은 모든 정보를 포함하지 않을 수 있는데, 이 경우 추가적인 정보를 습득하는 것이 필요하다. (특정 측면이나 어떤 세부사항에 대해서)

이 문제를 해결하기 위해 나왔던 여러 방법이 있는데
1. 정해진 주기마다 retrieval.
	근데 요거는 별로 안좋음. 필요할 때에 retrieve 할 수 없을 뿐만 아니라 retrieve의 의도를 제대로 반영하지 못함.
2. 전체 질문을 몇개의 sub-question으로 나누어서 동작. 각각은 추가적인 정보를 retrieve하기 위해 사용됨.

1. 답변을 생성하는 과정에서 언제, 어떻게 정보를 retrieve 할 지 결정하는 간단하고 포괄적인 Retrieval Argumented LM을 만들 수 있는가?
2. long-form generation task에 적용할 수 있는가?


언제 retrieve 하느냐? => Lack of required knowledge.

LM은 보통 잘 교정되지만, 낮은 확률/확신을 갖는다. 언제? Lack of knowledge.
active retrieval strategy => LM이 low confidence token을 생성했을 때.

when to retrieve는 결정됐는데 그러면 what to retrieve는?
=> retrieve 한 이후에 답변할 내용에 beneficial 한 게 중요.

그래서 나오는게 FLARE(Forword-Looking Active REtrieval augmented generation)
얘는 그리고 기존의 LM에 추가적인 학습을 하지 않고도 적용할 수 있음

document corpus $D_{q_t}$ = $\{d_i\}^{|\mathcal{D}|}_{i=1}$ : $q_t$ 의 쿼리 결과로 반환되는 documents의 리스트.
output $y = [s_1, s_2, ... , s_m] = [w_1, w_2, ... , w_n]$ : seq of sentence 혹은 seq of tokens
$y$는 LM이 생성한 문장이므로, $y=LM([\mathcal{D}_q, x])$로 정의할 수 있다.

### Active Retrieval Augmented Generation
언제, 무엇을 Retrieve 할 것인지를 결정하는 Generic한 framework
새로운 query $q_t$는 $x$와 이전까지의 답변 $y_{<t} = [y_0, y_1, ... , y_{t-1}]$에 의해 만들어진다.
ex) $q_t = qry(x, t_{<t})$
=> 이 쿼리의 결과로 retrive 한 documents의 List를 $\mathcal{D}_{q_t}$라 하면,
$y_t = LM([\mathcal{D}_{q_t}, x, y_{<t}])$로 생성할 수 있다.
=> 이렇게 새로운 문장, 혹은 토큰을 생성하는 경우에, $y_t$는 이전까지 생성한 토큰들, input $x$를 반영하게 된다.
(t step에서 새로운 토큰을 생성할 때, 이전까지 사용했던 $\mathcal{D}_{q_{<t}}$는 버리고 새로운 document list $\mathcal{D}_{q_t}$만을 사용하여 생성한다.

### FLARE : Forword-Looking Active REtrieval augmented generation
1) LM은 답변에 필요한 knowledge 만을 retrieve 해야함.
	- 부적절한 문장 생성을 피하기 위해.
2) retrieval query는 생성하고자 하는 문장의 의도를 반영해야함.

FLARE에는 두 가지 종류가 있음
1. $FLARE_{instruct}$
	- retrieve 가 필요할 때, instruction을 생성해서 query.
2. $FLARE_{direct}$
	- retrieve가 필요할 때, 지금까지 생성한 문장으로 query.

### FLARE w/ Retrieval Instructions
정보를 얻을 때, [Search(query)]를 만드는 것을 필요로 함.
해당 쿼리들은 추후의 생성에 대해서 관련된 정보를 retrieve 하는 것을 목표로 함.
[Search(query)]가 나타나면 생성과정을 중지하고 관련된 문서를 받아옴. 다시 문장을 생성함.
그러다가 또 쿼리가 발생하면 관련 문서를 찾아서 가져오는 과정을 반복함.
이 과정을 답변 생성이 끝날 때까지 반복함.

### Direct FLARE 
블랙박스 LM을 파인튜닝 할 수 없기 때문에 $FLARE_{instruct}$를 이용해 쿼리 검색을 했었음.
근데 조금 더 직관적인 방법이 있음. 

#### Confidence-based Active Retrieval
step $t$에서, 다음의 임시 문장 $\hat{s}_t = LM([x, y_{<t}])$ 을 생성한다.
$\hat{s}_t$의 토큰들 중 어떤 하나라도 확률이 $\theta$ 보다 작다면 $\hat{s}_t$를 기반으로 search query $q_t$를 생성한다.
생성한 쿼리로 검색을 실시한 후, 다시 생성 과정을 실시한다.
식으로 나타내면 다음과 같다.
 $y_t = \begin{cases} \hat{s}_t & \text{if all tokens of } \hat{s}_t \text{ have probs } \geq \theta \\ s_t = \text{LM}([D_{q_t}, x, y_{<t}]) & \text{otherwise} \end{cases}$


#### Confidence-based Query Formulation
새로 생성된 문장 $\hat{s}_t$를 쿼리로 사용하는 것이 더 좋은 결과를 반환한다는 연구 결과
그러나 $\hat{s}_t$는 잘못되었을 가능성이 있음, 이 경우 정확한 결과를 반환하지 못하게 됨.
이 문제를 해결하기 위한 두 가지 방법이 있음.
1. 확률이 $\beta$ 보다 낮은 모든 토큰 $\hat{s}_t$를 마스킹한 문장으로 query.
2. $\hat{s}_t$ 중에서 신뢰도가 낮은 부분을 대상으로 질문을 생성함.
	1. $\hat{s}_t$에서 신뢰도가 낮은 부분을 추출하여 $z$로 정의.
	2. $z$에 대해 프롬프트하여 질문 $q_{t, z}$를 생성.

$q_t = \begin{cases} \emptyset & \text{if all tokens of } \hat{s}_t \text{ have probs } \geq \theta \\ \text{mask}(\hat{s}_t) \text{ or } q_{\text{gen}}(\hat{s}_t) & \text{otherwise} \end{cases}$


### Multi-time Retrieval Baseline
1. Previous-sentence
2. Question decomposition
